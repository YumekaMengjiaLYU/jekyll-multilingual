---
layout: post
title:  "Planning by Dynamic Programming"
ref: welcome
date:   2020-06-15
tags: reinforcement-learning-lectures
lang: en
---
## Introduction

### What is dynamic programming?
- Dynamic sequential or temporal component to the problem
- Programming optimizing a program, i.e. a policy (say start with a mapping, optimize the behavior)
    - c.f. linear programming

Put the two ideas together ;)
- A method for solving complex problems
- By breaking them down into subproblems
    - Solve the subproblems

### Requirements for Dynamic Programming
Dynamic programming is a very general solution method for problems which have two properties:
- Optimal substructure
    - Principle of optimality applied
    - Optimal solution can be decomposed into subproblems
- Overlapping subproblems
    - Subproblems recur many times
- Markov decision process satisfy both properties
    - Bellman equation gives recursive decomposition
    - Value function stories and reuses solutions

### Planning by Dynamic Programming
- Dynamic programming assumes full knowledge of the MDP
- It is used for planning in an MDP
- For prediction:
    - Input: MDP $\langle \mathcal{S},\mathcal{A},\mathcal{P},\mathcal{R},\mathcal{\gamma} \rangle$ and policy $\pi$
    - or: MRP $\langle \mathcal{S},\mathcal{P^{\pi}},\mathcal{R^{\pi}}, \gamma$
    - Output: value function $v_\pi$
- Or for control:
    - Input: MDP $\langle \mathcal{S},\mathcal{A},\mathcal{P},\mathcal{R},\mathcal{\gamma} \rangle$
    - insteqd of given policy, want to know what the best policy solving mdp what is the best thing to do under the mdp best mapping from state to action achieve the long term reward in mdp
    - Output: optimal value function $v_\pi$
    - and equivalently: optimal policy $\pi_*$ 

### Not only used in MDPs
Dynamic programming is used to solve many other problems, e.g.
optimal substructure
- Scheduling algorithms
- String algorithms (e.g. sequence alignment)
- Graph algorithms (e.g. shortest path algorithms)
- Graphical models (e.g. Viterbi algorithm)
- Bioinformatics (e.g. lattice models)
### Iterative Policy Evaluation
- Problem: evaluate a given policy $\pi$
- Solution: iterate application of Bellman expectation backup
Bellman equation to do policy evaluation
Bellman optimization to do control
Take the Bellman equation and turn it into iterative update: our first mechanism for evaluating policies

- $v_1 \rightarrow v_2 \dots \rightarrow v_\pi$
one step lookhead using Bellman equation to find out v_2
- Using synchronous backups/evaluations, 
    - At each iteration k + 1,
    - For all states $s \in S$
    - Update $v_{k+1}(s)$ from $v_k(s')$
    - where s' is a successor state of s
- We will discuss asynchronous backups laters
- Convergence to $v_\pi$ will be proven at the end of the lecture
 
### Iterative Policy Evaluation - continued
$$
v_{k+1}(s) = \sum \pi(a|s) \left(\mathcal{R}_s^a + \gamma \sum \mathcal{R}_s^a v_k(s') \right)
$$
## Policy Iteration

## Value Iteration

## Extensions to Dynamic Programming

## Contraction Mapping